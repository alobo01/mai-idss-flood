{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f77fff1",
   "metadata": {},
   "source": [
    "# ðŸŒŠ Flood-IDSS: Multi-Horizon Real-Time Forecast\n",
    "This notebook demonstrates the full power of the IDSS. It loads models for **24h, 48h, and 72h** horizons to provide a complete risk outlook.\n",
    "\n",
    "### Dashboard Capabilities:\n",
    "1. **Live Data:** Fetches real-time river/weather data (USGS/Open-Meteo).\n",
    "2. **Multi-Horizon Forecast:** Predicts river levels for +1, +2, and +3 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab59981a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T13:11:09.911596Z",
     "start_time": "2025-12-02T13:11:06.718752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "LEAD_TIMES = [1, 2, 3] # Predict for 24h, 48h, 72h\n",
    "FLOOD_THRESHOLD = 30.0\n",
    "\n",
    "print(\"Libraries loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7edc818",
   "metadata": {},
   "source": [
    "## 1. Load All Trained Models\n",
    "We load the LSTM (Quantile 90) models for all three time horizons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1579f5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T13:11:10.138457Z",
     "start_time": "2025-12-02T13:11:09.914933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Loaded 1-Day Model (36 features)\n",
      "  âœ… Loaded 2-Day Model (36 features)\n",
      "  âœ… Loaded 3-Day Model (36 features)\n"
     ]
    }
   ],
   "source": [
    "# Custom Quantile Loss\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.maximum(q * (y_true - y_pred), (q - 1) * (y_true - y_pred)))\n",
    "\n",
    "MODELS = {}\n",
    "SCALERS = {}\n",
    "FEATURES = {}\n",
    "\n",
    "print(\"Loading Models...\")\n",
    "\n",
    "for dt in LEAD_TIMES:\n",
    "    try:\n",
    "        path = f\"../Results/L{dt}d\"\n",
    "        model_path = f\"{path}/models/lstm_q90.h5\"\n",
    "\n",
    "        # Load Model\n",
    "        m = load_model(model_path, custom_objects={'<lambda>': lambda y, p: quantile_loss(0.90, y, p)})\n",
    "\n",
    "        # Load Scalers\n",
    "        sx = joblib.load(f\"{path}/models/lstm_scaler_x.pkl\")\n",
    "        sy = joblib.load(f\"{path}/models/lstm_scaler_y.pkl\")\n",
    "\n",
    "        # Load Feature Names (Critical for mapping)\n",
    "        train_cols = pd.read_csv(f\"../Data/processed/L{dt}d/train.csv\", nrows=0).columns.tolist()\n",
    "        excl = ['date', 'time', 'target_level_max', 'target_level_mean', \n",
    "                'target_level_min', 'target_level_std', 'target_level', \n",
    "                'is_flood', 'is_major_flood']\n",
    "        feats = [c for c in train_cols if c not in excl]\n",
    "\n",
    "        MODELS[dt] = m\n",
    "        SCALERS[dt] = (sx, sy)\n",
    "        FEATURES[dt] = feats\n",
    "\n",
    "        print(f\"  âœ… Loaded {dt}-Day Model ({len(feats)} features)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed to load {dt}-Day Model: {e}\")\n",
    "        print(f\"     (Have you run 'run_full_experiment.py' for --days {dt}?)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ccf2f",
   "metadata": {},
   "source": [
    "## 2. Real-Time Data Fetcher\n",
    "Queries external APIs for the last 35 days of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4101a2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T13:11:10.154030Z",
     "start_time": "2025-12-02T13:11:10.139855Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_live_data():\n",
    "    print(\"ðŸ“¡ Fetching live data (last 7 days river, last 35 days weather)...\")\n",
    "\n",
    "    # Dates are still needed for Weather API\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=35)\n",
    "    str_start = start_date.strftime(\"%Y-%m-%d\")\n",
    "    str_end = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # --- 1. Weather (St. Louis) ---\n",
    "    # Open-Meteo still requires explicit dates\n",
    "    print(\"   â˜ï¸  Weather API...\")\n",
    "    url_w = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params_w = {\n",
    "        \"latitude\": 38.6270, \"longitude\": -90.1994,\n",
    "        \"start_date\": str_start, \"end_date\": str_end,\n",
    "        \"daily\": [\"precipitation_sum\"], \"timezone\": \"UTC\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r_w = requests.get(url_w, params=params_w).json()\n",
    "        df_w = pd.DataFrame({\n",
    "            'date': pd.to_datetime(r_w['daily']['time']),\n",
    "            'daily_precip': r_w['daily']['precipitation_sum']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Weather API Failed: {e}\")\n",
    "        df_w = pd.DataFrame({'date': pd.date_range(str_start, str_end), 'daily_precip': 0.0})\n",
    "\n",
    "    # --- 2. Rivers (USGS) - USING PERIOD 'P35D' ---\n",
    "    stations = {'target': '07010000', 'hermann': '06934500', 'grafton': '05587450'}\n",
    "    dfs_river = {}\n",
    "\n",
    "    for name, site in stations.items():\n",
    "        print(f\"   ðŸŒŠ {name.capitalize()} Station ({site})...\")\n",
    "\n",
    "        # Default fallback (Empty DF with NaNs)\n",
    "        # We create a placeholder index so the merge doesn't fail if API fails\n",
    "        dfs_river[name] = pd.DataFrame({'date': pd.date_range(str_start, str_end), f'{name}_level': np.nan})\n",
    "\n",
    "        try:\n",
    "            # URL for Instantaneous Values\n",
    "            url_r = \"https://waterservices.usgs.gov/nwis/iv/\"\n",
    "\n",
    "            # Params: period=P35D gets exactly the last 35 days of 15-min data\n",
    "            params_r = {\n",
    "                'format': 'json',\n",
    "                'sites': site,\n",
    "                'period': 'P7D',  # Last 35 Days\n",
    "                'parameterCd': '00065' # Gage Height\n",
    "            }\n",
    "\n",
    "            r_r = requests.get(url_r, params=params_r).json()\n",
    "\n",
    "            # Robust Parsing\n",
    "            if 'value' in r_r and 'timeSeries' in r_r['value'] and r_r['value']['timeSeries']:\n",
    "                vals_list = r_r['value']['timeSeries'][0]['values'][0]['value']\n",
    "\n",
    "                if vals_list:\n",
    "                    # 1. Load 15-min data\n",
    "                    temp_df = pd.DataFrame(vals_list)\n",
    "                    temp_df['value'] = pd.to_numeric(temp_df['value'])\n",
    "                    temp_df['dateTime'] = pd.to_datetime(temp_df['dateTime']).dt.tz_localize(None)\n",
    "\n",
    "                    # 2. Resample to Daily Mean (to match training data)\n",
    "                    temp_df['date'] = temp_df['dateTime'].dt.floor('D')\n",
    "                    daily_mean = temp_df.groupby('date')['value'].mean().reset_index()\n",
    "\n",
    "                    # 3. Store result\n",
    "                    dfs_river[name] = pd.DataFrame({\n",
    "                        'date': daily_mean['date'],\n",
    "                        f'{name}_level': daily_mean['value']\n",
    "                    })\n",
    "                    print(f\"      âœ“ Got {len(daily_mean)} daily averages\")\n",
    "                else:\n",
    "                    print(f\"      âš ï¸ Data list is empty for {name}\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ No timeSeries found for {name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      âš ï¸ Connection error for {name}: {e}\")\n",
    "\n",
    "    # --- 3. Merge ---\n",
    "    df_merged = df_w.copy()\n",
    "    for name in stations:\n",
    "        # Outer merge ensures we keep the dates even if one source is missing\n",
    "        df_merged = df_merged.merge(dfs_river[name], on='date', how='left')\n",
    "\n",
    "    # Clean up dates and sort\n",
    "    df_merged = df_merged.sort_values('date').reset_index(drop=True)\n",
    "    df_merged = df_merged.rename(columns={'target_level': 'target_level_max'})\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611457d4",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "Standard cleaning: Interpolation for river gaps, 0.0 for missing rain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39f4a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T13:11:12.131083Z",
     "start_time": "2025-12-02T13:11:10.159182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Fetching live data (last 7 days river, last 35 days weather)...\n",
      "   â˜ï¸  Weather API...\n",
      "      âŒ Weather API Failed: Could not find a suitable TLS CA certificate bundle, invalid path: /Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/certifi/cacert.pem\n",
      "   ðŸŒŠ Target Station (07010000)...\n",
      "      âš ï¸ Connection error for target: Could not find a suitable TLS CA certificate bundle, invalid path: /Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/certifi/cacert.pem\n",
      "   ðŸŒŠ Hermann Station (06934500)...\n",
      "      âš ï¸ Connection error for hermann: Could not find a suitable TLS CA certificate bundle, invalid path: /Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/certifi/cacert.pem\n",
      "   ðŸŒŠ Grafton Station (05587450)...\n",
      "      âš ï¸ Connection error for grafton: Could not find a suitable TLS CA certificate bundle, invalid path: /Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/certifi/cacert.pem\n",
      "ðŸ§¹ Cleaning Data...\n",
      "   ðŸš¨ TOTAL API FAILURE for target_level_max. Using emergency fallback (15.0 ft).\n",
      "   ðŸš¨ TOTAL API FAILURE for hermann_level. Using emergency fallback (15.0 ft).\n",
      "   ðŸš¨ TOTAL API FAILURE for grafton_level. Using emergency fallback (15.0 ft).\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df_raw):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    print(\"ðŸ§¹ Cleaning Data...\")\n",
    "\n",
    "    # 1. Weather\n",
    "    df['daily_precip'] = df['daily_precip'].fillna(0.0)\n",
    "\n",
    "    # 2. Rivers\n",
    "    river_cols = [c for c in df.columns if 'level' in c]\n",
    "    for col in river_cols:\n",
    "        if df[col].isna().all():\n",
    "            print(f\"   ðŸš¨ TOTAL API FAILURE for {col}. Using emergency fallback (15.0 ft).\")\n",
    "            df[col] = 15.0 \n",
    "        else:\n",
    "            na_count = df[col].isna().sum()\n",
    "            if na_count > 0:\n",
    "                df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "                print(f\"   - Interpolated {na_count} gaps in {col}\")\n",
    "\n",
    "    # 3. Final Fallback\n",
    "    df = df.ffill().bfill()\n",
    "    return df\n",
    "\n",
    "# Execute Fetch & Clean\n",
    "raw_history = fetch_live_data()\n",
    "clean_history = preprocess_data(raw_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f691a",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Engine\n",
    "This function maps \"Today's Data\" to the specific feature requirements of each model (1d, 2d, 3d).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15ed9f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T13:11:12.144580Z",
     "start_time": "2025-12-02T13:11:12.132577Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_features(df_raw, required_features):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # 1. Calculate Rolling/Cumulative Features\n",
    "    df['precip_7d'] = df['daily_precip'].rolling(7).sum()\n",
    "    df['precip_14d'] = df['daily_precip'].rolling(14).sum()\n",
    "    df['precip_30d'] = df['daily_precip'].rolling(30).sum()\n",
    "    df['soil_deep_30d'] = 0.5 \n",
    "\n",
    "    for w in [3, 7, 14]:\n",
    "        df[f'hermann_ma{w}d'] = df['hermann_level'].rolling(w).mean()\n",
    "        df[f'grafton_ma{w}d'] = df['grafton_level'].rolling(w).mean()\n",
    "\n",
    "    df = df.ffill().bfill()\n",
    "    today = df.iloc[-1]\n",
    "\n",
    "    # 2. Map to Model Features\n",
    "    input_data = {}\n",
    "    for feat in required_features:\n",
    "        # Direct Lags (Maps Today -> LagN)\n",
    "        if 'hermann_lag' in feat: input_data[feat] = today['hermann_level']\n",
    "        elif 'grafton_lag' in feat: input_data[feat] = today['grafton_level']\n",
    "        elif 'target_lag' in feat: input_data[feat] = today['target_level_max']\n",
    "\n",
    "        # Weather\n",
    "        elif 'precip_7d' in feat: input_data[feat] = today['precip_7d']\n",
    "        elif 'precip_14d' in feat: input_data[feat] = today['precip_14d']\n",
    "        elif 'precip_30d' in feat: input_data[feat] = today['precip_30d']\n",
    "        elif 'daily_precip' in feat: input_data[feat] = today['daily_precip']\n",
    "        elif 'soil_deep' in feat: input_data[feat] = 0.5\n",
    "\n",
    "        # Rolling Stats (Robust matching)\n",
    "        elif 'hermann_ma' in feat: \n",
    "            if '3d' in feat: input_data[feat] = today['hermann_ma3d']\n",
    "            elif '7d' in feat: input_data[feat] = today['hermann_ma7d']\n",
    "            elif '14d' in feat: input_data[feat] = today['hermann_ma14d']\n",
    "\n",
    "        elif 'grafton_ma' in feat:\n",
    "            if '3d' in feat: input_data[feat] = today['grafton_ma3d']\n",
    "            elif '7d' in feat: input_data[feat] = today['grafton_ma7d']\n",
    "            elif '14d' in feat: input_data[feat] = today['grafton_ma14d']\n",
    "\n",
    "        else:\n",
    "            input_data[feat] = 0.0\n",
    "\n",
    "    return pd.DataFrame([input_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c77621b",
   "metadata": {},
   "source": [
    "## 5. Live Forecast (1-3 Days)\n",
    "Predicting future levels for St. Louis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0bc7fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T13:11:12.858099Z",
     "start_time": "2025-12-02T13:11:12.153235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸŒŠ ST. LOUIS RIVER FORECAST (Current Level: 15.00 ft)\n",
      "============================================================\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "  File \"/var/folders/rn/5sc5vrvn6yl8jbwyfj4_51f40000gn/T/ipykernel_44195/2149708606.py\", line 18, in <module>\n",
      "    pred = scaler_y.inverse_transform(model.predict(X_s, verbose=0))[0][0]\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1012, in range\n",
      "ImportError: cannot import name 'range_op' from 'tensorflow.python.data.ops' (/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/tensorflow/python/data/ops/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1114, in get_records\n",
      "  File \"/Users/elhamidi/Desktop/mai-idss-flood/venv/lib/python3.9/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(f\"ðŸŒŠ ST. LOUIS RIVER FORECAST (Current Level: {clean_history.iloc[-1]['target_level_max']:.2f} ft)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for dt in LEAD_TIMES:\n",
    "    if dt not in MODELS: continue\n",
    "\n",
    "    # 1. Get components\n",
    "    model = MODELS[dt]\n",
    "    scaler_x, scaler_y = SCALERS[dt]\n",
    "    feats = FEATURES[dt]\n",
    "\n",
    "    # 2. Prepare & Predict\n",
    "    X = prepare_features(clean_history, feats)\n",
    "    X_s = scaler_x.transform(X).reshape((1, 1, len(feats)))\n",
    "    pred = scaler_y.inverse_transform(model.predict(X_s, verbose=0))[0][0]\n",
    "\n",
    "    # 3. Status\n",
    "    status = \"âš ï¸ FLOOD RISK\" if pred > FLOOD_THRESHOLD else \"âœ… SAFE\"\n",
    "    color = \"red\" if pred > FLOOD_THRESHOLD else \"green\"\n",
    "\n",
    "    print(f\"T + {dt} Days:  {pred:.2f} ft  [{status}]\")\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "days = [f\"T+{d}d\" for d in LEAD_TIMES if d in MODELS]\n",
    "plt.bar(days, predictions, color=['skyblue', 'steelblue', 'darkblue'])\n",
    "plt.axhline(FLOOD_THRESHOLD, color='red', linestyle='--', label='Flood Stage (30ft)')\n",
    "plt.ylabel(\"River Level (ft)\")\n",
    "plt.title(\"Forecast Horizon\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
